{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10937911,"sourceType":"datasetVersion","datasetId":6801996}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport numpy as np\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T11:16:55.726761Z","iopub.execute_input":"2025-03-28T11:16:55.727037Z","iopub.status.idle":"2025-03-28T11:16:55.731348Z","shell.execute_reply.started":"2025-03-28T11:16:55.727016Z","shell.execute_reply":"2025-03-28T11:16:55.730332Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Directories\ntrain_occluded_dir = '/kaggle/input/bedroom-occluded-images/train/occluded_images'\ntrain_original_dir = '/kaggle/input/bedroom-occluded-images/train/original_images'\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T10:01:58.086806Z","iopub.execute_input":"2025-03-28T10:01:58.087091Z","iopub.status.idle":"2025-03-28T10:01:58.090877Z","shell.execute_reply.started":"2025-03-28T10:01:58.087070Z","shell.execute_reply":"2025-03-28T10:01:58.089941Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class BedroomDataset(Dataset):\n    def __init__(self, occluded_dir, original_dir, transform=None):\n        self.occluded_dir = occluded_dir\n        self.original_dir = original_dir\n        self.transform = transform\n        \n        # Match occluded to original using the numeric identifiers\n        self.occluded_files = sorted(os.listdir(occluded_dir), key=lambda x: int(x.split('_')[-1].split('.')[0]))\n        self.original_files = sorted(os.listdir(original_dir), key=lambda x: int(x.split('_')[-1].split('.')[0]))\n        \n        # Ensure both directories contain matching pairs\n        self.image_pairs = [(os.path.join(occluded_dir, o), os.path.join(original_dir, r)) \n                            for o, r in zip(self.occluded_files, self.original_files) \n                            if int(o.split('_')[-1].split('.')[0]) == int(r.split('_')[-1].split('.')[0])]\n\n    def __len__(self):\n        return len(self.image_pairs)\n\n    def __getitem__(self, idx):\n        occluded_path, original_path = self.image_pairs[idx]\n        \n        occluded_image = Image.open(occluded_path).convert(\"RGB\")\n        original_image = Image.open(original_path).convert(\"RGB\")\n\n        if self.transform:\n            occluded_image = self.transform(occluded_image)\n            original_image = self.transform(original_image)\n\n        return occluded_image, original_image\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T11:17:15.976201Z","iopub.execute_input":"2025-03-28T11:17:15.976562Z","iopub.status.idle":"2025-03-28T11:17:15.983263Z","shell.execute_reply.started":"2025-03-28T11:17:15.976532Z","shell.execute_reply":"2025-03-28T11:17:15.982603Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Define paths (update these if needed)\ntrain_occluded_dir = \"/kaggle/input/bedroom-occluded-images/train/occluded_images\"\ntrain_original_dir = \"/kaggle/input/bedroom-occluded-images/train/original_images\"\n\n# Image Transformations\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n])\n\n# Load Dataset\ntrain_dataset = BedroomDataset(train_occluded_dir, train_original_dir, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n\nprint(f\"Total matched pairs: {len(train_dataset)}\")\n\n# Check shape of a batch\nsample_occluded, sample_original = next(iter(train_loader))\nprint(\"Sample batch shape (occluded):\", sample_occluded.shape)\nprint(\"Sample batch shape (original):\", sample_original.shape)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T11:49:05.766365Z","iopub.execute_input":"2025-03-28T11:49:05.766753Z","iopub.status.idle":"2025-03-28T11:49:05.988325Z","shell.execute_reply.started":"2025-03-28T11:49:05.766723Z","shell.execute_reply":"2025-03-28T11:49:05.987623Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport os\n\nclass ConvLSTMCell(nn.Module):\n    def __init__(self, input_dim, hidden_dim, kernel_size):\n        super(ConvLSTMCell, self).__init__()\n        self.input_dim = input_dim\n        self.hidden_dim = hidden_dim\n\n        padding = kernel_size // 2\n        self.conv = nn.Conv2d(input_dim + hidden_dim, hidden_dim * 4, kernel_size, padding=padding)\n\n    def forward(self, x, hidden):\n        h_cur, c_cur = hidden\n\n        combined = torch.cat([x, h_cur], dim=1)\n        conv_output = self.conv(combined)\n\n        i, f, o, g = torch.chunk(conv_output, 4, dim=1)\n\n        i = torch.sigmoid(i)      # Input gate\n        f = torch.sigmoid(f)      # Forget gate\n        o = torch.sigmoid(o)      # Output gate\n        g = torch.tanh(g)         # Cell state\n\n        c_next = f * c_cur + i * g\n        h_next = o * torch.tanh(c_next)\n\n        return h_next, c_next\n\n\nclass OptimizedPixelRNN(nn.Module):\n    def __init__(self):\n        super(OptimizedPixelRNN, self).__init__()\n\n        # Encoder: Deeper for better feature extraction\n        self.encoder = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=5, stride=2, padding=2),  # [B, 64, 128, 128]\n            nn.ReLU(),\n            nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1),  # [B, 128, 64, 64]\n            nn.ReLU(),\n            nn.Conv2d(128, 256, kernel_size=3, stride=2, padding=1),  # [B, 256, 32, 32]\n            nn.ReLU()\n        )\n\n        # ConvLSTM\n        self.convlstm = ConvLSTMCell(input_dim=256, hidden_dim=256, kernel_size=3)\n\n        # Decoder: PixelShuffle for smoother upscaling\n        self.decoder = nn.Sequential(\n            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.PixelShuffle(2),  # [B, 128, 64, 64]\n            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.PixelShuffle(2),  # [B, 64, 128, 128]\n            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n            nn.ReLU(),\n            nn.PixelShuffle(2),  # [B, 32, 256, 256]\n            nn.Conv2d(32, 3, kernel_size=3, padding=1),\n            nn.Sigmoid()  # Ensure output is in range [0, 1]\n        )\n\n    def forward(self, x):\n        batch_size = x.size(0)\n        device = x.device\n\n        # Encoding\n        x = self.encoder(x)  # [B, 256, 32, 32]\n\n        # Initialize hidden and cell states\n        h, c = (torch.zeros(batch_size, 256, 32, 32, device=device),\n                torch.zeros(batch_size, 256, 32, 32, device=device))\n\n        # ConvLSTM step\n        h, c = self.convlstm(x, (h, c))\n\n        # Decoding\n        x = self.decoder(h)  # [B, 3, 256, 256]\n        return x\n\n\n# Model setup\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\nmodel = OptimizedPixelRNN().to(device)\n\n# Dummy input to test the model\ntorch.cuda.empty_cache()  # Clear CUDA cache\nsample_input = torch.randn(8, 3, 256, 256).to(device)\n\nwith torch.no_grad():\n    output = model(sample_input)\n\nprint(\"Model output shape:\", output.shape)  # Expected: [8, 3, 256, 256]\n\n# Ensure no cuDNN errors due to input contiguity\nprint(\"Input contiguous:\", sample_input.is_contiguous())\nprint(\"Output contiguous:\", output.is_contiguous())\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T12:41:45.204976Z","iopub.execute_input":"2025-03-28T12:41:45.205329Z","iopub.status.idle":"2025-03-28T12:41:46.502202Z","shell.execute_reply.started":"2025-03-28T12:41:45.205306Z","shell.execute_reply":"2025-03-28T12:41:46.501348Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport os\nfrom piq import ssim\n\n# Hyperparameters\nnum_epochs = 20\nlearning_rate = 0.001\nsave_dir = \"/kaggle/working\"\n\n# Loss and Optimizer\nmse_loss = nn.MSELoss()  # Pixel-wise reconstruction loss\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Combined Loss Function (MSE + SSIM)\ndef combined_loss(outputs, targets, alpha=0.8):\n    # MSE Loss (Pixel-wise accuracy)\n    mse = mse_loss(outputs, targets)\n    \n    # SSIM Loss (Structural Similarity)\n    ssim_loss = 1 - ssim(outputs, targets, data_range=1.0)\n    \n    # Weighted sum of both losses\n    return alpha * mse + (1 - alpha) * ssim_loss\n\n# Training Loop\nfor epoch in range(num_epochs):\n    model.train()\n    total_loss = 0.0\n\n    for occluded_images, original_images in train_loader:\n        occluded_images, original_images = occluded_images.to(device), original_images.to(device)\n\n        # Forward pass\n        outputs = model(occluded_images)\n\n        # Compute combined loss (MSE + SSIM)\n        loss = combined_loss(outputs, original_images)\n\n        # Backward pass and optimization\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {avg_loss:.4f}\")\n\n# Save model\nmodel_path = os.path.join(save_dir, \"optimized_pixelrnn.pth\")\ntorch.save(model.state_dict(), model_path)\nprint(f\"Model saved to {model_path}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T12:41:54.914960Z","iopub.execute_input":"2025-03-28T12:41:54.915266Z","iopub.status.idle":"2025-03-28T12:47:41.775358Z","shell.execute_reply.started":"2025-03-28T12:41:54.915243Z","shell.execute_reply":"2025-03-28T12:47:41.774368Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"save_dir= \"/kaggle/working\"\nmodel_path = os.path.join(save_dir, \"optimized_pixelrnn.pth\")\ntorch.save(model.state_dict(), model_path)\nprint(f\"Model saved to {model_path}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T11:59:49.312092Z","iopub.execute_input":"2025-03-28T11:59:49.312474Z","iopub.status.idle":"2025-03-28T11:59:49.347843Z","shell.execute_reply.started":"2025-03-28T11:59:49.312444Z","shell.execute_reply":"2025-03-28T11:59:49.346859Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport torch\nimport torchvision.transforms as transforms\nfrom torch.utils.data import Dataset, DataLoader\nfrom PIL import Image\nimport os\n\n# Path to test images\ntest_occluded_dir = \"/kaggle/input/bedroom-occluded-images/occluded_test\"\n\n# Ensure device compatibility\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# Image Transform\ntransform = transforms.Compose([\n    transforms.Resize((256, 256)),\n    transforms.ToTensor(),\n])\n\n# Custom Dataset for Occluded Test Images\nclass OccludedTestDataset(Dataset):\n    def __init__(self, image_dir, transform=None):\n        self.image_dir = image_dir\n        self.transform = transform\n        self.image_files = sorted(os.listdir(image_dir))\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.image_dir, self.image_files[idx])\n        image = Image.open(img_path).convert(\"RGB\")\n        if self.transform:\n            image = self.transform(image)\n        return image, self.image_files[idx]\n\n# Load test data\ntest_dataset = OccludedTestDataset(test_occluded_dir, transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)\n\n# Load trained model\nmodel = OptimizedPixelRNN().to(device)\nmodel.load_state_dict(torch.load(\"/kaggle/working/optimized_pixelrnn.pth\", map_location=device))\nmodel.eval()\n\n# Evaluate and visualize\ndef evaluate_and_visualize(model, test_loader, device):\n    model.to(device)\n    model.eval()\n\n    with torch.no_grad():\n        for i, (occluded_images, filenames) in enumerate(test_loader):\n            occluded_images = occluded_images.to(device)\n\n            # Generate reconstructions\n            reconstructed_images = model(occluded_images)\n\n            # Ensure values are in valid range [0, 1]\n            reconstructed_images = torch.clamp(reconstructed_images, 0, 1)\n\n            # Move tensors to CPU for visualization\n            occluded_images = occluded_images.cpu()\n            reconstructed_images = reconstructed_images.cpu()\n\n            # Display results\n            for j in range(occluded_images.size(0)):\n                fig, axs = plt.subplots(1, 2, figsize=(8, 4))\n\n                axs[0].imshow(occluded_images[j].permute(1, 2, 0))\n                axs[0].set_title(f\"Occluded: {filenames[j]}\")\n                axs[0].axis(\"off\")\n\n                axs[1].imshow(reconstructed_images[j].permute(1, 2, 0))\n                axs[1].set_title(\"Reconstructed\")\n                axs[1].axis(\"off\")\n\n                plt.show()\n\n# Run evaluation\nevaluate_and_visualize(model, test_loader, device)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-28T12:48:12.619222Z","iopub.execute_input":"2025-03-28T12:48:12.619605Z","iopub.status.idle":"2025-03-28T12:49:00.528525Z","shell.execute_reply.started":"2025-03-28T12:48:12.619577Z","shell.execute_reply":"2025-03-28T12:49:00.527432Z"}},"outputs":[],"execution_count":null}]}